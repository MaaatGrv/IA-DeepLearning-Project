{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des Matières\n",
    "\n",
    "1. [Introduction](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#Introduction)\n",
    "2. [Étape 1 : Importer les Bibliothèques Nécessaires](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-1-Importer-les-Biblioth%C3%A8ques-N%C3%A9cessaires)\n",
    "3. [Étape 2 : Définir les Fonctions de Prétraitement et d’Augmentation](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-2-D%C3%A9finir-les-Fonctions-de-Pr%C3%A9traitement-et-d%E2%80%99Augmentation)\n",
    "4. [Étape 3 : Définir la Fonction de Création des Modèles](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-3-D%C3%A9finir-la-Fonction-de-Cr%C3%A9ation-des-Mod%C3%A8les)\n",
    "5. [Étape 4 : Définir la Fonction de Téléchargement Audio depuis YouTube](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-4-D%C3%A9finir-la-Fonction-de-T%C3%A9l%C3%A9chargement-Audio-depuis-YouTube)\n",
    "6. [Étape 5 : Initialiser les Hyperparamètres](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-5-Initialiser-les-Hyperparam%C3%A8tres)\n",
    "7. [Étape 6 : Charger et Prétraiter les Données](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-6-Charger-et-Pr%C3%A9traiter-les-Donn%C3%A9es)\n",
    "8. [Étape 7 : Itérer sur les Combinaisons d'Hyperparamètres et Entraîner les Modèles](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-7-It%C3%A9rer-sur-les-Combinaisons-d'Hyperparam%C3%A8tres-et-Entra%C3%AEner-les-Mod%C3%A8les)\n",
    "9. [Étape 8 : Analyser et Sauvegarder les Résultats](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-8-Analyser-et-Sauvegarder-les-R%C3%A9sultats)\n",
    "10. [Étape 9 : Visualiser les Meilleurs Résultats](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-9-Visualiser-les-Meilleurs-R%C3%A9sultats)\n",
    "11. [Étape 10 : Sauvegarder et Charger les Meilleurs Modèles](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-10-Sauvegarder-et-Charger-les-Meilleurs-Mod%C3%A8les)\n",
    "12. [Étape 11 : Prédiction sur de Nouveaux Échantillons](https://chatgpt.com/c/6755b38f-32a8-8010-b45f-f9ddc4e36e80#%C3%89tape-11-Pr%C3%A9diction-sur-de-Nouveaux-%C3%89chantillons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "\n",
    "1. **Charger et prétraiter les données audio** en extrayant les caractéristiques MFCC avec augmentation de données.\n",
    "2. **Définir et entraîner différents modèles** (CNN 2D, CNN profond, RNN avec LSTM/GRU) en testant diverses combinaisons d'hyperparamètres tels que la taille du batch, le nombre d'époques, le taux d'apprentissage et le nombre de coefficients MFCC.\n",
    "3. **Évaluer les performances** de chaque modèle et collecter les métriques dans un tableau pour une analyse comparative.\n",
    "4. **Visualiser les meilleurs résultats** et **sauvegarder les modèles** optimaux pour une utilisation future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 1 : Importer les Bibliothèques Nécessaires\n",
    "\n",
    "Tout d’abord, importons toutes les bibliothèques nécessaires pour le prétraitement des données, la création des modèles, et l’évaluation des performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import joblib\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten, LSTM, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import yt_dlp as youtube_dl\n",
    "\n",
    "# Pour afficher les graphiques directement dans le notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 2 : Définir les Fonctions de Prétraitement et d’Augmentation\n",
    "\n",
    "Nous allons définir les fonctions nécessaires pour :\n",
    "\n",
    "- **Augmenter les données audio** (ajout de bruit, modification de la hauteur et de la vitesse).\n",
    "- **Padder ou tronquer les MFCCs** pour assurer une longueur fixe.\n",
    "- **Extraire les MFCCs** avec augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio, sr):\n",
    "    \"\"\"\n",
    "    Applique des augmentations sur l'audio : ajout de bruit, modification de la hauteur et de la vitesse.\n",
    "    \"\"\"\n",
    "    # Ajout de bruit\n",
    "    noise = np.random.randn(len(audio)) * 0.005\n",
    "    audio_with_noise = audio + noise\n",
    "\n",
    "    # Modifier la hauteur en fréquence\n",
    "    try:\n",
    "        pitch_factor = np.random.uniform(-5, 5)  # En demi-tons\n",
    "        audio_pitch_shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_factor)\n",
    "    except:\n",
    "        audio_pitch_shifted = audio\n",
    "\n",
    "    # Modifier la vitesse\n",
    "    try:\n",
    "        audio_stretched = librosa.effects.time_stretch(audio, rate=np.random.uniform(0.8, 1.2))\n",
    "    except:\n",
    "        audio_stretched = audio\n",
    "\n",
    "    return [audio_with_noise, audio_pitch_shifted, audio_stretched]\n",
    "\n",
    "def pad_or_truncate_mfcc(mfccs, fixed_length):\n",
    "    \"\"\"\n",
    "    Ajuste la longueur des MFCCs (n_mfcc, T) à (n_mfcc, fixed_length)\n",
    "    - Tronque si T > fixed_length\n",
    "    - Ajoute du padding (zéros) si T < fixed_length\n",
    "    \"\"\"\n",
    "    length = mfccs.shape[1]\n",
    "    if length > fixed_length:\n",
    "        mfccs = mfccs[:, :fixed_length]\n",
    "    elif length < fixed_length:\n",
    "        pad_width = fixed_length - length\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mfccs\n",
    "\n",
    "def extract_features_with_augmentation(file_path, fixed_length, n_mfcc=40):\n",
    "    \"\"\"\n",
    "    Extrait les MFCCs d'un fichier audio avec augmentation de données.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, duration=30)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs = pad_or_truncate_mfcc(mfccs, fixed_length)\n",
    "\n",
    "        # Augmentations\n",
    "        augmented_audios = augment_audio(audio, sr)\n",
    "        augmented_mfccs = []\n",
    "        for a in augmented_audios:\n",
    "            m = librosa.feature.mfcc(y=a, sr=sr, n_mfcc=n_mfcc)\n",
    "            m = pad_or_truncate_mfcc(m, fixed_length)\n",
    "            augmented_mfccs.append(m)\n",
    "\n",
    "        # On renvoie la liste (original + augmentations) directement sous forme (n_mfcc, T)\n",
    "        all_mfccs = [mfccs] + augmented_mfccs\n",
    "        return all_mfccs\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement du fichier {file_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 3 : Définir la Fonction de Création des Modèles\n",
    "\n",
    "Nous allons définir une fonction qui crée différents types de modèles en fonction des hyperparamètres spécifiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, input_shape, num_classes, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Crée un modèle de deep learning en fonction du type spécifié.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    if model_type == 'CNN_2D_basic':\n",
    "        # Modèle CNN 2D de base\n",
    "        model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    elif model_type == 'CNN_2D_deep':\n",
    "        # Modèle CNN 2D plus profond\n",
    "        model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    elif model_type == 'RNN_LSTM':\n",
    "        # Modèle avec couches LSTM\n",
    "        # Reshape pour LSTM : (N, T, n_mfcc)\n",
    "        model.add(Flatten(input_shape=input_shape))  # Convertir en séquence pour LSTM\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(LSTM(128, return_sequences=True))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(LSTM(64))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    elif model_type == 'RNN_GRU':\n",
    "        # Modèle avec couches GRU\n",
    "        # Reshape pour GRU : (N, T, n_mfcc)\n",
    "        model.add(Flatten(input_shape=input_shape))  # Convertir en séquence pour GRU\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(GRU(128, return_sequences=True))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(GRU(64))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Type de modèle inconnu: {model_type}\")\n",
    "\n",
    "    # Compilation du modèle\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4 : Définir la Fonction de Téléchargement Audio depuis YouTube\n",
    "\n",
    "Cette fonction permet de télécharger un fichier audio depuis une URL YouTube pour tester les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_youtube_audio(youtube_url, output_path=\"downloaded_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Télécharge l'audio d'une vidéo YouTube et le sauvegarde en fichier WAV.\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': 'temp_audio.%(ext)s',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "    }\n",
    "\n",
    "    # Télécharger l'audio\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "\n",
    "    # Renommer le fichier pour l'utiliser\n",
    "    if os.path.exists(\"temp_audio.wav\"):\n",
    "        os.rename(\"temp_audio.wav\", output_path)\n",
    "        print(f\"Audio téléchargé et sauvegardé sous : {output_path}\")\n",
    "    else:\n",
    "        print(\"Erreur lors du téléchargement.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 5 : Initialiser les Hyperparamètres\n",
    "\n",
    "Définissons les différentes valeurs des hyperparamètres que nous allons tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des hyperparamètres à tester\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs_list = [20, 50, 100]\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "model_types = ['CNN_2D_basic', 'CNN_2D_deep', 'RNN_LSTM', 'RNN_GRU']  # Ajouter d'autres modèles si nécessaire\n",
    "n_mfcc_values = [20, 40, 60]  # \"token size\" interprété comme nombre de MFCCs\n",
    "\n",
    "# Préparation du tableau de résultats\n",
    "results = []\n",
    "\n",
    "# Créer un répertoire pour sauvegarder les modèles\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 6 : Charger et Prétraiter les Données\n",
    "\n",
    "Pour chaque valeur de `n_mfcc`, nous chargerons et prétraiterons les données audio avec augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_features_with_augmentation(data_path, fixed_length, n_mfcc=40):\n",
    "    \"\"\"\n",
    "    Charge les features audio avec augmentation et ajuste les MFCCs à une longueur fixe.\n",
    "    \"\"\"\n",
    "    genres = os.listdir(data_path)\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for genre in genres:\n",
    "        genre_path = os.path.join(data_path, genre)\n",
    "        for file_name in os.listdir(genre_path):\n",
    "            file_path = os.path.join(genre_path, file_name)\n",
    "            mfccs_list = extract_features_with_augmentation(file_path, fixed_length=fixed_length, n_mfcc=n_mfcc)\n",
    "            for m in mfccs_list:\n",
    "                # m est de forme (n_mfcc, fixed_length)\n",
    "                X_list.append(m)\n",
    "                y_list.append(genre)\n",
    "\n",
    "    X = np.array(X_list)  # X: (N, n_mfcc, fixed_length)\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # Ajout de la dimension \"canal\" pour le CNN 2D : (N, n_mfcc, fixed_length, 1)\n",
    "    X = X[..., np.newaxis]\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 7 : Itérer sur les Combinaisons d'Hyperparamètres et Entraîner les Modèles\n",
    "\n",
    "Nous allons maintenant itérer sur toutes les combinaisons possibles d'hyperparamètres et de types de modèles, entraîner chaque modèle, évaluer ses performances, et enregistrer les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers les données audio\n",
    "data_path = \"Data/genres_original\"\n",
    "\n",
    "# Définir fixed_length en fonction de n_mfcc et des paramètres de vos MFCC\n",
    "fixed_length = 1293  # À ajuster selon votre dataset (en fonction du sr et hop_length)\n",
    "\n",
    "# Itération sur les différentes valeurs de n_mfcc (token size)\n",
    "for n_mfcc in n_mfcc_values:\n",
    "    print(f\"\\nTraitement pour n_mfcc = {n_mfcc}\")\n",
    "\n",
    "    # Charger les données avec augmentation\n",
    "    X, y = load_audio_features_with_augmentation(data_path, fixed_length, n_mfcc=n_mfcc)\n",
    "\n",
    "    print(f\"Shape des données: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "    # Encodage des labels\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded, num_classes=len(encoder.classes_))\n",
    "\n",
    "    # Split des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Normalisation des données\n",
    "    X_train = X_train / np.max(X_train)\n",
    "    X_test = X_test / np.max(X_test)\n",
    "\n",
    "    # Itération sur les différentes combinaisons d'hyperparamètres\n",
    "    for model_type, batch_size, epochs, lr in itertools.product(model_types, batch_sizes, epochs_list, learning_rates):\n",
    "        print(f\"Entraînement du modèle: {model_type}, Batch Size: {batch_size}, Epochs: {epochs}, Learning Rate: {lr}\")\n",
    "\n",
    "        # Définir le modèle\n",
    "        input_shape = X_train.shape[1:]  # (n_mfcc, fixed_length, 1)\n",
    "        num_classes = len(encoder.classes_)\n",
    "        model = create_model(model_type, input_shape, num_classes, learning_rate=lr)\n",
    "\n",
    "        # Définir le callback EarlyStopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Optionnel : Définir un ModelCheckpoint pour sauvegarder le meilleur modèle\n",
    "        model_save_path = f\"models/n_mfcc_{n_mfcc}/{model_type}_bs{batch_size}_ep{epochs}_lr{lr}.h5\"\n",
    "        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "        checkpoint = ModelCheckpoint(model_save_path, monitor='val_accuracy', save_best_only=True, verbose=0)\n",
    "\n",
    "        # Entraîner le modèle\n",
    "        history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs,\n",
    "                            batch_size=batch_size, callbacks=[early_stopping, checkpoint], verbose=0)  # verbose=0 pour moins de sorties\n",
    "\n",
    "        # Évaluer le modèle\n",
    "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "        # Enregistrer les résultats\n",
    "        result = {\n",
    "            'n_mfcc': n_mfcc,\n",
    "            'model_type': model_type,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': epochs,\n",
    "            'learning_rate': lr,\n",
    "            'accuracy': accuracy,\n",
    "            'loss': loss,\n",
    "            'validation_accuracy': val_accuracy,\n",
    "            'validation_loss': val_loss\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Résultat: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarques :**\n",
    "\n",
    "- **Boucles imbriquées** : Nous itérons d'abord sur les différentes valeurs de `n_mfcc`, puis sur toutes les combinaisons d'hyperparamètres pour chaque type de modèle.\n",
    "- **Sauvegarde des Modèles** : Utilisation de `ModelCheckpoint` pour sauvegarder le meilleur modèle basé sur la précision de validation (`val_accuracy`) pour chaque combinaison d'hyperparamètres.\n",
    "- **Normalisation** : Les MFCCs sont normalisés en les divisant par leur maximum pour faciliter la convergence du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 8 : Analyser et Sauvegarder les Résultats\n",
    "\n",
    "Après avoir entraîné tous les modèles, nous allons collecter les résultats dans un tableau et les sauvegarder dans un fichier CSV pour une analyse ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les résultats en DataFrame et sauvegarder\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(\"\\nTous les résultats ont été sauvegardés dans 'model_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 9 : Visualiser les Meilleurs Résultats\n",
    "\n",
    "Nous allons afficher les 10 meilleures configurations basées sur la précision de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les meilleurs résultats\n",
    "best_results = results_df.sort_values(by='accuracy', ascending=False).head(10)\n",
    "print(\"\\nTop 10 des meilleures configurations :\")\n",
    "display(best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des Résultats\n",
    "\n",
    "Pour mieux comprendre les performances des différents modèles et hyperparamètres, nous pouvons créer des visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de la précision en fonction du modèle et du taux d'apprentissage pour chaque n_mfcc\n",
    "for n_mfcc in n_mfcc_values:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    subset = results_df[results_df['n_mfcc'] == n_mfcc]\n",
    "    pivot_table = subset.pivot_table(values='accuracy', index='model_type', columns='learning_rate', aggfunc='mean')\n",
    "    sns.heatmap(pivot_table, annot=True, fmt=\".4f\", cmap='viridis')\n",
    "    plt.title(f\"Précision moyenne par modèle et taux d'apprentissage (n_mfcc={n_mfcc})\")\n",
    "    plt.ylabel(\"Type de Modèle\")\n",
    "    plt.xlabel(\"Taux d'Apprentissage\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot de la précision par type de modèle\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='model_type', y='accuracy', data=results_df)\n",
    "plt.title(\"Distribution de la Précision par Type de Modèle\")\n",
    "plt.xlabel(\"Type de Modèle\")\n",
    "plt.ylabel(\"Précision\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot de la précision en fonction du taux d'apprentissage pour chaque modèle\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.scatterplot(x='learning_rate', y='accuracy', hue='model_type', style='n_mfcc', data=results_df)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Précision en fonction du Taux d'Apprentissage par Type de Modèle\")\n",
    "plt.xlabel(\"Taux d'Apprentissage (log scale)\")\n",
    "plt.ylabel(\"Précision\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 10 : Sauvegarder et Charger les Meilleurs Modèles\n",
    "\n",
    "Pour réutiliser les meilleurs modèles sans avoir à les réentraîner, nous pouvons les sauvegarder et les charger ultérieurement.\n",
    "\n",
    "### Sauvegarde des Meilleurs Modèles\n",
    "\n",
    "Les modèles sont déjà sauvegardés pendant l'entraînement grâce au callback `ModelCheckpoint`. Assurez-vous que les chemins des modèles sont correctement définis dans les résultats.\n",
    "\n",
    "### Chargement d'un Modèle Sauvegardé\n",
    "\n",
    "Voici comment charger un modèle sauvegardé et l'utiliser pour faire des prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de chargement d'un modèle sauvegardé\n",
    "best_model_path = 'models/n_mfcc_40/CNN_2D_deep_bs32_ep50_lr0.001.h5'  # Remplacez par le chemin de votre meilleur modèle\n",
    "best_model = load_model(best_model_path)\n",
    "\n",
    "# Charger l'encodeur des labels (si non déjà chargé)\n",
    "encoder = joblib.load('label_encoder.joblib')\n",
    "\n",
    "# Fonction de prédiction\n",
    "def predict_genre(file_path, model, encoder, fixed_length=1293, n_mfcc=40):\n",
    "    \"\"\"\n",
    "    Prédit le genre musical d'un fichier audio.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, duration=30)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfccs = pad_or_truncate_mfcc(mfccs, fixed_length)\n",
    "        mfccs = mfccs[..., np.newaxis]  # Ajouter la dimension canal\n",
    "        mfccs = np.expand_dims(mfccs, axis=0)  # Ajouter la dimension batch\n",
    "\n",
    "        # Normalisation\n",
    "        mfccs = mfccs / np.max(mfccs)\n",
    "\n",
    "        # Prédiction\n",
    "        predicted_genre_index = np.argmax(model.predict(mfccs), axis=-1)\n",
    "        predicted_genre = encoder.inverse_transform(predicted_genre_index)[0]\n",
    "        return predicted_genre\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la prédiction : {e}\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
